# K-Nearest Neighbor(K-NN)

새로운 데이터로부터 **가장 가까운 k개**의 기준 데이터를 찾아 **가장 많이 분류된 값(다수결)** 으로 현재의 데이터를 분류(보통 k는 **홀수** 값으로 설정)

- 가장 가까운 데이터를 기준으로 새로운 데이터의 클래스를 분류
- 새로운 입력 데이터와 가장 가까운 k개의 이웃 데이터 선택
- 다수결로 데이터의 클래스 결정



## K-NN 알고리즘 계산 순서

1. 새로운 데이터 입력
2. 거리 계산하기
3. k개의 이웃 찾기
4. 이웃 레이블에 기반한 데이터 분류



## K-NN 알고리즘을 활용한 분류

1. 이진 분류 
   - 두 가지 중 하나로 분류
   - Ex) 악성 코드 분류, 위조 지표 분류, 사람의 감정 행복 또는 슬픔 분류
2. 다중 분류 
   - 여러 개의 가능한 레이블 중 하나로 분류
   - kNN 알고리즘은 다중 분류에도 탁월한 성능을 보임
   - ex) 1 ~ 9 중 가장 까가운 숫자, 서울 여러 도시 분류, 사람의 다양한 감정 분류



## k 값의 수

k값이 너무 작으면 **과대적합(overfitting)** 이 될 확률이 높음, k 값이 너무 크면 **과소적합(underfitting)** 이 될 확률이 높음



## 점 사이의 거리 공식

- N 가지 속성은 n차원 공간에 표현 가능
- n차원 공간에서 거리는 피타고라스 정리를 적용(유클리디안 거리)

$ \sqrt{\sum_{k=1}^{n}(x_{k})^2}=y_n $



## k-NN 알고리즘의 장단점

### 장점

- 원리가 간단하여 구현하기 쉬움
- 수치 기반 데이터 분류에 높은 성능
- 별도의 학습 모델이 필요 없음(데이터가 들어 갈 때 마다 계산)

### 단점

- 에측 속도가 느림
- 고차원 데이터 처리를 위해 차원 축소가 필요
- k 값 선정에 따라 성능이 좌우



## 실습 코드



